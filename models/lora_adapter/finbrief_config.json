{
  "base_model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  "lora_r": 16,
  "lora_alpha": 32,
  "lora_dropout": 0.1,
  "target_modules": null,
  "num_epochs": 2,
  "batch_size": 2,
  "learning_rate": 0.0002,
  "warmup_steps": 100,
  "max_seq_length": 512,
  "gradient_accumulation_steps": 4,
  "output_dir": "models/lora_adapter",
  "logging_dir": "logs/lora_training",
  "eval_steps": 50,
  "save_steps": 100
}